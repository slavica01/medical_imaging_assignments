{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Assignment 1: Deadline: first class in week 4 of the course: 25-2-2025 10:00 (if updated).\n",
    "Hand in this notebook with output. Make sure that it is able to run and produce all the figures and results you show. Also, use the text boxes to answer the questions and interpret your results, relating them to the course materials.\n",
    "\n",
    "\n",
    "Exercises made by Oliver Gurney-Champion. Please contact us via Canvas, or e-mail directly to:\n",
    "Oliver: o.j.gurney-champion@amsterdamumc.nl\n",
    "Matthan: m.w.a.caan@amsterdamumc.nl\n",
    "Dilara: d.tank@amsterdamumc.nl\n",
    "Daan: d.kuppens@amsterdamumc.nl\n",
    "\n",
    "These are a large set of challenging exercises, for which you will get 3 weeks to complete. I would strongly advise you to stick to the suggested schedule, which will ensure you have sufficient knowledge to answer the questions when completing them, and finalize all questions in time.\n",
    "\n",
    "Note that the networks will be lite and can run on your local computer/laptop in short time (minutes). There is no need as yet to run this on Surf, although we highly encourage you to make sure Surf works for you (for exercise sets 2 and 3).\n",
    "\n",
    "# Exercise 1: Program network in PyTorch. (60%)\n",
    "During the class, a brief introduction was given to quantitative imaging. In this exercise, you will program your first neural network that will help estimate quantitative MRI parameters from quantitative data. In particular, we will be looking at the intra-voxel incoherent motion (IVIM) model for diffusion-weighted MRI:\n",
    "\n",
    "S(b)=S_0×( (1-f)×e^-b×D^ +f×e^-b×D*^ )                                                                    [1]\n",
    "\n",
    "With S the measured signal, S0 the baseline signal at S(b=0), f the perfusion fraction, D the diffusion coefficient and D* the pseudo diffusion coefficient. For more information on what the model means exactly and how it is used clinically, I would suggest reading \"Introduction to IVIM MRI | Radiology Key\" (https://radiologykey.com/introduction-to-ivim-mri/). But for the purpose of this exercise, it is just a model.\n",
    "\n",
    "Normally, f, D and D* (named Dp in the code) are obtained by fitting S(b) using least-squares fitting. But these approaches are known to be prone to noise in the data and often produce poor estimates.\n",
    "\n",
    "Therefore, you will write a neural network that predicts f from a given S(b). There are great tools available that take care of training models, such as PyTorch, Karas and Tensorflow. However, for the purpose of this exercise, you will make use of PyTorch. Moreover, we will use weights-and-biasses to keep track of how training is going.\n",
    "\n",
    "# Wednesday 3-4-2024\n",
    "At https://github.com/oliverchampion/AI_for_medical_imaging_course you will find the Python assignment. To help visualize progress and to isolate certain snippets of code, we wrote this as a Jupyter Notebook (exercise1.ipynb). As you can see, we have already provided a data-generator, some plotting tools to plot the training progress. The notebook should run as is and train a neural network! For your first lecture, we suggest you (Wednesday)\n",
    "-\tInstall all prerequisites in your virtual enviroment (requierements.txt)\n",
    "-\tStart a WandB account @ https://wandb.ai/  you will need to log in when running the script\n",
    "-\tGo through the script to see whether you understand what happens.\n",
    "-\tTrain your first neural network .\n",
    "-\tVisualize the results on your WandB page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import requiered packages\n",
    "imports the packages and sets the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import helper_functions as hf\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set random seed\n",
    "seed =42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "login to your free wandb account. Note you will need to set up your account on https://wandb.ai/authorize\n",
    "wandb allows you to keep track of your neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "%env WANDB_SILENT=True\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Simulate and view the IVIM data\n",
    "This allows you to study what the data looks like in jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x259f7bd6180>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set b-values at which we \"measure\" (i.e. simulate signal)\n",
    "bvalues=[0, 10, 20, 30, 40, 50, 75, 100, 150, 250, 400, 600]\n",
    "\n",
    "## Set the random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "## Loading the (simulated) dataset\n",
    "data_sim, D, f, Dp = hf.sim_signal(SNR=(10,30),bvalues=bvalues,sims=30,seed=np.random.randint(1,10000))\n",
    "\n",
    "## plotting some curves and data for visualisation\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axs[i, j].plot(bvalues, data_sim[i+10*j,:], 'o')\n",
    "        datapred=hf.ivim(np.arange(0,np.max(bvalues)), D[i+10*j], f[i+10*j], Dp[i+10*j], 1)\n",
    "        axs[i, j].plot(np.arange(0,np.max(bvalues)), datapred)\n",
    "        axs[i, j].set_ylim(0, 1.2)\n",
    "        axs[i, j].set(xlabel='b-value (s/mm2)', ylabel='normalised signal')\n",
    "plt.legend(('noisy data', 'true curve'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Splitting the data into training and validation\n",
    "Here, we split our data into a training set, validation set and test set. Note that the current implementation only uses the training set and it is up to you (in your exercises) to also implement the validation and test run. At this point, we already split the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x259f7d609e0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x259f7d63260>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x259fa11e5a0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sim_dat(bvalues,batch_size = 16,SNR=(10,40),sims=1000,seed=np.random.randint(1,10000)):\n",
    "    with wandb.init(project=\"AI_for_medical_imaging\", job_type=\"visualize data\") as run:\n",
    "        data_sim = hf.sim_signal(SNR=SNR,bvalues=bvalues,sims=sims,seed=seed)\n",
    "        # Only for visualisation purposes: here we create our \"Artifact\" in wandb --> this allows viewing the data in your wandb account\n",
    "        for i in range(4):\n",
    "            #make b-value data pairs\n",
    "            example_data=[[x,y] for (x,y) in zip(bvalues,data_sim[0][i])]\n",
    "            # put it in a table\n",
    "            table = wandb.Table(data=example_data, columns=[\"b-values\", \"signal\"])\n",
    "            #tell wandb to plot the table\n",
    "            wandb.log({\"data_plot \" + str(i): wandb.plot.scatter(table, \"b-values\", \"signal\")})\n",
    "\n",
    "        # here we split the data into train (70%), test (15%) and validation (15%) sets\n",
    "        #split = int(np.floor(len(data_sim[0]) * 0.7))\n",
    "        train_set, test_set, val_set = torch.utils.data.random_split([[data_sim[0][i,:],data_sim[1][i],data_sim[2][i],data_sim[3][i]] for i in range(len(data_sim[3]))],[0.7,0.15,0.15])\n",
    "        #split = int(np.floor(len(rest) * 0.5))\n",
    "        #test_set, val_set = torch.utils.data.random_split([[rest[0][i,:],rest[1][i],rest[2][i],rest[3][i]] for i in range(len(rest[3]))],[split, len(rest[0]) - split])\n",
    "\n",
    "        # train loader loads the trianing data. We want to shuffle to make sure data order is modified each epoch and different data is selected each epoch.\n",
    "        trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                       drop_last=True)\n",
    "        # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "        inferloader = torch.utils.data.DataLoader(val_set,\n",
    "                                       batch_size=min(batch_size,len(val_set)),\n",
    "                                       shuffle=False,\n",
    "                                       drop_last=False)\n",
    "            # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "        testloader = torch.utils.data.DataLoader(test_set,\n",
    "                                       batch_size=min(batch_size,len(test_set)),\n",
    "                                       shuffle=False,\n",
    "                                       drop_last=False)\n",
    "    return trainloader, inferloader, testloader\n",
    "sim_dat(bvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Design a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# funtion for stacking the layers and making the model\n",
    "def make_model(n_inputs=5,n_hidden=1,n_outputs=1):\n",
    "    #initialize the model object\n",
    "    model = nn.Sequential()\n",
    "    # fill out the model with hidden layers.\n",
    "    for i in range(n_hidden):\n",
    "        # as we loop, we add hidden layers\n",
    "        model.add_module('layer_linear'+str(i), nn.Linear(n_inputs, n_inputs))\n",
    "        # we also add a ReLu layer\n",
    "        model.add_module('layer_ReLu'+str(i), nn.ReLU())\n",
    "    #and a final output layer\n",
    "    model.add_module('last_layer',nn.Linear(n_inputs, n_outputs))\n",
    "    # to ensure positive predictions, we end with a ReLu function before giving output\n",
    "    model.add_module('last',nn.Sigmoid())\n",
    "    model.apply(init_weights)\n",
    "    return model\n",
    "\n",
    "# function for initializing network weights for individual layers\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train your first network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def train_network(name, epochs=200, learningrate=0.1, hidden_layers=2, seed=42, optimizer='SGD',bvalues=bvalues,batch_size=16,sims=1000):\n",
    "\n",
    "    trainloader, inferloader, testloader = sim_dat(bvalues,batch_size=batch_size,sims=sims)\n",
    "\n",
    "    model = make_model(n_inputs=len(bvalues), n_hidden=hidden_layers, n_outputs=1)\n",
    "\n",
    "    # initialize model --> we did this above, but during the exercise, you might be re-running this part of the script several times with different settings. This way we make sure you re-initiate the training and don't continue in the last model\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    # initialize wandb\n",
    "    wandb.init(\n",
    "            project=\"AI_for_medical_imaging\", job_type=\"training\", name=name)\n",
    "\n",
    "    # set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # probe available devices\n",
    "    if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.determinstic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Set default device. If GPU is available, the network will be trained on the GPU. Note that further down in the code, stuff will be sent \".to(device)\" to make sure it is available on the GPU.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # define the loss of the network (mean square error)\n",
    "    loss_module = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "    # the optimizer determines how strongly to update the network's weights based on the calculated loss.\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learningrate)\n",
    "    else:\n",
    "        raise NotImplementedError('this optimizer is not implemented yet...')\n",
    "\n",
    "    # loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # initiate losses to 0\n",
    "        train_loss_f = 0\n",
    "        val_loss_f = 0\n",
    "        # set model to training such that forward passes are remembered (requiered for backpropogating the loss)\n",
    "        model.train()\n",
    "        #loop over all training data0\n",
    "        SD_train = 0\n",
    "        sys_train = 0\n",
    "        for x in trainloader:\n",
    "            # reset the gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            # get data (x[0]) and put the data on the GPU if available\n",
    "            batch=x[0].to(device)\n",
    "            # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "            f_ref = torch.flatten(x[2].to(device))\n",
    "            # put the data through the neural network\n",
    "            f_pred = torch.flatten(model.forward(batch))\n",
    "            # calculate loss (compare predicted f to the ground trueth)\n",
    "            loss_f = loss_module(f_pred, f_ref)\n",
    "            #add found loss to the train loss, to keep track of the loss this epoch\n",
    "            train_loss_f += loss_f.item()\n",
    "            # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "            loss_f.backward()\n",
    "            # update all weights accoording to their derrivatives to the loss.\n",
    "            optimizer.step()\n",
    "            # calculate the standard deviation and systematic error on the trianing data\n",
    "            SD, sys = hf.error_metrics(f_pred.cpu().detach().numpy(),f_ref.cpu().detach().numpy())\n",
    "            # add the errors to ultimately calculate their mean over the trianing data. calculating mean SDs goes via the Root Mean Squares. So add SDs squared\n",
    "            SD_train += SD**2\n",
    "            sys_train += sys\n",
    "        # now divide by the total amount of training data to calculate the mean (sys error) and square of mean (SD).\n",
    "        SD_train = np.sqrt(SD_train/trainloader.__len__())\n",
    "        sys_train = sys_train/trainloader.__len__()\n",
    "        # after training, set model to evaluation mode\n",
    "        model.eval()\n",
    "        # initialize error_metrics\n",
    "        SD_val=0\n",
    "        sys_val=0\n",
    "        ######################your code here for validation loss#########################\n",
    "        #make b-value data pairs: Note these currently contain the f_ref and f_pred from the trianing data. You may want to swap to validation data once implemented\n",
    "        example_data=[[x,y] for (x,y) in zip(f_ref.cpu().detach().numpy(),f_pred.cpu().detach().numpy())]\n",
    "        # put it in a table\n",
    "        table = wandb.Table(data=example_data, columns=[\"f_ref\", \"f_pred\"])\n",
    "        #tell wandb to plot the table\n",
    "        # note that some parameters are being logged which you still need to define in the validation loop!\n",
    "        if epoch % 10 == 0:\n",
    "            wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_train,\"error/systematic error\":sys_train,\"data_plot epoch \" + str(epoch): wandb.plot.scatter(table, \"f_ref\", \"f_pred\", title=f'epoch{epoch}')})\n",
    "\n",
    "        ## print output in terminal. Only useful for debugging when WandB does not work\n",
    "        #print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))\n",
    "    wandb.finish()\n",
    "    #return val_test SD_test\n",
    "\n",
    "train_network('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# First week (on schedule means finish by Friday) [if updated: 7-2-2025]\n",
    "For these exercises, you will want to produce new cells that generate the outputs. The notebook cells should run and produce the figures without input of the examiners.\n",
    "\n",
    "## 1A-C: train the neural network\n",
    "Adapt the script above for your exercises A-C such that it produces the desired results and plots. Describe the results/intepertation in this text baloon.\n",
    "\n",
    "A.\tThe current network implementation only looks at training data. This means that the network’s performance is over-estimated. Please use the validation set to monitor performance during training (note that we have already put the model on evaluation mode in line 59). At what point is the network fully trained? Explain how you know this. Show the effect from overfitting and underfitting.\n",
    "B.\tSimilarly, use the test dataset to test for final performance. Explain why this is needed.\n",
    "C.\tCurrently, standard gradient descent optimizer is being used to train the network, with a learning rate of 0.01. Investigate the performance of the network for different  optimizer (i.e. adam loss was discussed in the lecture) and explain what you see. What does the Adam  optimizer do differently from the SGD  optimizer that would make it perform better/differently?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 0.03727858387973419, Val Loss = 0.034394165687263015, Sys Error = -0.002095569856464863, Rand Error = 0.18272724747657776\n",
      "Epoch 1: Train Loss = 0.032936114298049796, Val Loss = 0.03244000021368265, Sys Error = -0.020463142544031143, Rand Error = 0.17640432715415955\n",
      "Epoch 2: Train Loss = 0.03069225919627866, Val Loss = 0.03015769738703966, Sys Error = -0.006030396558344364, Rand Error = 0.1711287945508957\n",
      "Epoch 3: Train Loss = 0.02844679113044295, Val Loss = 0.028105697967112063, Sys Error = 0.0037831738591194153, Rand Error = 0.1652805358171463\n",
      "Epoch 4: Train Loss = 0.02608069300998089, Val Loss = 0.025571266282349826, Sys Error = -0.006288829259574413, Rand Error = 0.1576511412858963\n",
      "Epoch 5: Train Loss = 0.023638686728338864, Val Loss = 0.023331998288631438, Sys Error = 0.006075817160308361, Rand Error = 0.15064147114753723\n",
      "Epoch 6: Train Loss = 0.02123036031979461, Val Loss = 0.021261850837618113, Sys Error = 0.01452210359275341, Rand Error = 0.1432652473449707\n",
      "Epoch 7: Train Loss = 0.018991904300745835, Val Loss = 0.019135975558310748, Sys Error = 0.01715199276804924, Rand Error = 0.13560305535793304\n",
      "Epoch 8: Train Loss = 0.01675710948400719, Val Loss = 0.016845936048775913, Sys Error = 0.00866420567035675, Rand Error = 0.1280026137828827\n",
      "Epoch 9: Train Loss = 0.014789828348385041, Val Loss = 0.01520365634933114, Sys Error = -0.010704349726438522, Rand Error = 0.12150067090988159\n",
      "Epoch 10: Train Loss = 0.013138132234818713, Val Loss = 0.013784138113260269, Sys Error = 0.0007687166798859835, Rand Error = 0.11611081659793854\n",
      "Epoch 11: Train Loss = 0.012290231114643257, Val Loss = 0.012857490172609686, Sys Error = 0.009629628621041775, Rand Error = 0.11170748621225357\n",
      "Epoch 12: Train Loss = 0.011307234350635215, Val Loss = 0.012502096965909005, Sys Error = 0.022296831011772156, Rand Error = 0.10827425122261047\n",
      "Epoch 13: Train Loss = 0.010925117836788643, Val Loss = 0.01209388873539865, Sys Error = 0.025443127378821373, Rand Error = 0.10567577928304672\n",
      "Epoch 14: Train Loss = 0.010385975469077048, Val Loss = 0.011118153715506196, Sys Error = 0.006096747238188982, Rand Error = 0.10397718101739883\n",
      "Epoch 15: Train Loss = 0.010177320037253721, Val Loss = 0.010986803378909827, Sys Error = 0.014921980910003185, Rand Error = 0.1024051234126091\n",
      "Epoch 16: Train Loss = 0.00992574149656088, Val Loss = 0.010645234026014805, Sys Error = 0.009068635292351246, Rand Error = 0.10141226649284363\n",
      "Epoch 17: Train Loss = 0.009500181827650861, Val Loss = 0.010492287622764707, Sys Error = -0.00431545777246356, Rand Error = 0.10099087655544281\n",
      "Epoch 18: Train Loss = 0.009790478615350155, Val Loss = 0.010320717794820666, Sys Error = 0.0019589955918490887, Rand Error = 0.10017014294862747\n",
      "Epoch 19: Train Loss = 0.00967790081360659, Val Loss = 0.010232971562072635, Sys Error = 0.0008733171271160245, Rand Error = 0.09973261505365372\n",
      "Epoch 20: Train Loss = 0.009566650303557168, Val Loss = 0.010176272597163916, Sys Error = 0.007662819232791662, Rand Error = 0.0991189107298851\n",
      "Epoch 21: Train Loss = 0.009530080601486355, Val Loss = 0.010083560040220619, Sys Error = 0.0036562730092555285, Rand Error = 0.09887602180242538\n",
      "Epoch 22: Train Loss = 0.009520110295175813, Val Loss = 0.01005642144009471, Sys Error = 0.007789633236825466, Rand Error = 0.09847976267337799\n",
      "Epoch 23: Train Loss = 0.009242704333087732, Val Loss = 0.010291088558733463, Sys Error = 0.019747337326407433, Rand Error = 0.09796267002820969\n",
      "Epoch 24: Train Loss = 0.009317314999481274, Val Loss = 0.009979905374348164, Sys Error = -0.004306756425648928, Rand Error = 0.09831979870796204\n",
      "Epoch 25: Train Loss = 0.009119362407905418, Val Loss = 0.010007688240148128, Sys Error = -0.007843226194381714, Rand Error = 0.09824997931718826\n",
      "Epoch 26: Train Loss = 0.009328917129138527, Val Loss = 0.009858902683481574, Sys Error = 0.005272229202091694, Rand Error = 0.09762144088745117\n",
      "Epoch 27: Train Loss = 0.009164745892333083, Val Loss = 0.009841435542330145, Sys Error = 0.0077493274584412575, Rand Error = 0.09735534340143204\n",
      "Epoch 28: Train Loss = 0.009131437985139877, Val Loss = 0.009896223293617367, Sys Error = 0.013058106414973736, Rand Error = 0.09705483913421631\n",
      "Epoch 29: Train Loss = 0.008971537333414999, Val Loss = 0.010434660781174897, Sys Error = 0.028002161532640457, Rand Error = 0.0966312512755394\n",
      "Epoch 30: Train Loss = 0.009101518110350468, Val Loss = 0.009728132095187903, Sys Error = 0.006645523011684418, Rand Error = 0.0968499407172203\n",
      "Epoch 31: Train Loss = 0.008921113688238832, Val Loss = 0.00971512240357697, Sys Error = 0.008673605509102345, Rand Error = 0.09661364555358887\n",
      "Epoch 32: Train Loss = 0.008936645338571695, Val Loss = 0.01003585485741496, Sys Error = 0.02154969610273838, Rand Error = 0.09622590988874435\n",
      "Epoch 33: Train Loss = 0.009061517854502727, Val Loss = 0.009641347918659449, Sys Error = -0.002442028373479843, Rand Error = 0.09661626070737839\n",
      "Epoch 34: Train Loss = 0.008942931446485048, Val Loss = 0.009588872664608061, Sys Error = 0.00012014666572213173, Rand Error = 0.09636493772268295\n",
      "Epoch 35: Train Loss = 0.008871503564160924, Val Loss = 0.009557881159707904, Sys Error = 0.004484205506742001, Rand Error = 0.0960843414068222\n",
      "Epoch 36: Train Loss = 0.00890294223114155, Val Loss = 0.00952496042009443, Sys Error = 0.002392911585047841, Rand Error = 0.09599237143993378\n",
      "Epoch 37: Train Loss = 0.008833668784909817, Val Loss = 0.00949483176227659, Sys Error = 0.0023523829877376556, Rand Error = 0.09583238512277603\n",
      "Epoch 38: Train Loss = 0.008710493236173724, Val Loss = 0.009487377805635333, Sys Error = 0.006345619913190603, Rand Error = 0.09560281038284302\n",
      "Epoch 39: Train Loss = 0.008828752387185083, Val Loss = 0.009529112093150615, Sys Error = 0.01132353488355875, Rand Error = 0.09534423798322678\n",
      "Epoch 40: Train Loss = 0.008740855971091362, Val Loss = 0.009776150342077017, Sys Error = 0.02054433710873127, Rand Error = 0.09507622569799423\n",
      "Epoch 41: Train Loss = 0.008426964716162793, Val Loss = 0.009395906561985612, Sys Error = 0.005323584191501141, Rand Error = 0.09517941623926163\n",
      "Epoch 42: Train Loss = 0.008743460994040551, Val Loss = 0.009419535426422954, Sys Error = 0.009259136393666267, Rand Error = 0.09499528259038925\n",
      "Epoch 43: Train Loss = 0.00876750543626935, Val Loss = 0.009487194661051034, Sys Error = 0.013601777143776417, Rand Error = 0.09481453150510788\n",
      "Epoch 44: Train Loss = 0.00866388705786491, Val Loss = 0.009717613784596324, Sys Error = 0.02108929678797722, Rand Error = 0.09463082253932953\n",
      "Epoch 45: Train Loss = 0.00864008006228264, Val Loss = 0.009358853800222278, Sys Error = 0.009442136622965336, Rand Error = 0.09464633464813232\n",
      "Epoch 46: Train Loss = 0.008569094249545488, Val Loss = 0.009315762971527874, Sys Error = -0.004445337224751711, Rand Error = 0.09482227265834808\n",
      "Epoch 47: Train Loss = 0.008493469638186832, Val Loss = 0.00926981926895678, Sys Error = -0.001824735663831234, Rand Error = 0.09465713798999786\n",
      "Epoch 48: Train Loss = 0.008682279701366327, Val Loss = 0.009240403585135937, Sys Error = -0.00013013117131777108, Rand Error = 0.09451030194759369\n",
      "Epoch 49: Train Loss = 0.008613635450168404, Val Loss = 0.009350833809003234, Sys Error = 0.012546735815703869, Rand Error = 0.0942142903804779\n",
      "Epoch 50: Train Loss = 0.008574225241318345, Val Loss = 0.009240438044071198, Sys Error = 0.007441358175128698, Rand Error = 0.09418132156133652\n",
      "Epoch 51: Train Loss = 0.008376095419598007, Val Loss = 0.009195924596861004, Sys Error = -0.0016206061700358987, Rand Error = 0.09425241500139236\n",
      "Epoch 52: Train Loss = 0.008431369127424131, Val Loss = 0.009226709092035889, Sys Error = 0.009138254448771477, Rand Error = 0.09393942356109619\n",
      "Epoch 53: Train Loss = 0.008559958375773804, Val Loss = 0.00928709446452558, Sys Error = 0.01279437355697155, Rand Error = 0.09381742030382156\n",
      "Epoch 54: Train Loss = 0.008536778563676878, Val Loss = 0.009152462496422232, Sys Error = 0.005606372375041246, Rand Error = 0.09382583945989609\n",
      "Epoch 55: Train Loss = 0.00839799455215418, Val Loss = 0.00912338811904192, Sys Error = 0.004127237014472485, Rand Error = 0.09374655783176422\n",
      "Epoch 56: Train Loss = 0.008017921628062289, Val Loss = 0.009205985069274902, Sys Error = -0.008909063413739204, Rand Error = 0.09389600902795792\n",
      "Epoch 57: Train Loss = 0.008444236618531652, Val Loss = 0.009095071302726864, Sys Error = 0.00583815248683095, Rand Error = 0.09348567575216293\n",
      "Epoch 58: Train Loss = 0.008332141691299026, Val Loss = 0.009058120287954807, Sys Error = 0.0021002430003136396, Rand Error = 0.09345559030771255\n",
      "Epoch 59: Train Loss = 0.008228227577367148, Val Loss = 0.009044112311676145, Sys Error = -0.001216286327689886, Rand Error = 0.09340251237154007\n",
      "Epoch 60: Train Loss = 0.008126205101414301, Val Loss = 0.009077874664217233, Sys Error = -0.006205982528626919, Rand Error = 0.09339852631092072\n",
      "Epoch 61: Train Loss = 0.008361781909412076, Val Loss = 0.0090558429248631, Sys Error = 0.008082076907157898, Rand Error = 0.0930851399898529\n",
      "Epoch 62: Train Loss = 0.008338374822118948, Val Loss = 0.00898887631483376, Sys Error = 0.0006961238104850054, Rand Error = 0.09309569746255875\n",
      "Epoch 63: Train Loss = 0.00837859245244587, Val Loss = 0.009014428965747357, Sys Error = 0.007066760212182999, Rand Error = 0.09294914454221725\n",
      "Epoch 64: Train Loss = 0.008222608188123898, Val Loss = 0.009000416845083237, Sys Error = 0.006975960917770863, Rand Error = 0.09288075566291809\n",
      "Epoch 65: Train Loss = 0.008046406256251557, Val Loss = 0.009129985887557267, Sys Error = 0.013307841494679451, Rand Error = 0.09289202839136124\n",
      "Epoch 66: Train Loss = 0.008154972239808982, Val Loss = 0.00986595954746008, Sys Error = 0.02968926727771759, Rand Error = 0.09304167330265045\n",
      "Epoch 67: Train Loss = 0.00806651855710634, Val Loss = 0.008950797421857715, Sys Error = 0.00490477355197072, Rand Error = 0.09276381134986877\n",
      "Epoch 68: Train Loss = 0.008170933717168694, Val Loss = 0.009074069699272514, Sys Error = 0.0120324045419693, Rand Error = 0.09277485311031342\n",
      "Epoch 69: Train Loss = 0.0081481380593898, Val Loss = 0.009483618708327413, Sys Error = 0.023303132504224777, Rand Error = 0.09281615912914276\n",
      "Epoch 70: Train Loss = 0.008073362191532587, Val Loss = 0.008992687379941345, Sys Error = -0.009061027318239212, Rand Error = 0.09271076321601868\n",
      "Epoch 71: Train Loss = 0.008105710836473939, Val Loss = 0.009060319559648634, Sys Error = 0.012249951250851154, Rand Error = 0.09267658740282059\n",
      "Epoch 72: Train Loss = 0.008216995226089345, Val Loss = 0.009325856249779463, Sys Error = 0.02028622291982174, Rand Error = 0.09267452359199524\n",
      "Epoch 73: Train Loss = 0.0081975111645798, Val Loss = 0.00901253274641931, Sys Error = 0.010924025438725948, Rand Error = 0.09258407354354858\n",
      "Epoch 74: Train Loss = 0.008146890693582421, Val Loss = 0.008976189070381224, Sys Error = -0.009680444374680519, Rand Error = 0.09257175028324127\n",
      "Epoch 75: Train Loss = 0.00794006991849909, Val Loss = 0.008869939786382019, Sys Error = 0.0024289831053465605, Rand Error = 0.09243426471948624\n",
      "Epoch 76: Train Loss = 0.008031335128687842, Val Loss = 0.008893466554582119, Sys Error = -0.00569232739508152, Rand Error = 0.09243829548358917\n",
      "Epoch 77: Train Loss = 0.007991977396074596, Val Loss = 0.008964911522343754, Sys Error = 0.01046066451817751, Rand Error = 0.09236428141593933\n",
      "Epoch 78: Train Loss = 0.00800737066649247, Val Loss = 0.00884302044287324, Sys Error = 0.0016169908922165632, Rand Error = 0.09230201691389084\n",
      "Epoch 79: Train Loss = 0.008005953361388556, Val Loss = 0.00915992371737957, Sys Error = 0.017254967242479324, Rand Error = 0.09238334000110626\n",
      "Epoch 80: Train Loss = 0.007994279303306411, Val Loss = 0.008832866372540594, Sys Error = -0.0025862338952720165, Rand Error = 0.09223218262195587\n",
      "Epoch 81: Train Loss = 0.007919944830910238, Val Loss = 0.009599839989095926, Sys Error = 0.026858527213335037, Rand Error = 0.09244529157876968\n",
      "Epoch 82: Train Loss = 0.00808989777511289, Val Loss = 0.008833399880677462, Sys Error = 0.002366409171372652, Rand Error = 0.09223712980747223\n",
      "Epoch 83: Train Loss = 0.0077698942284684545, Val Loss = 0.008823022665455937, Sys Error = 0.002782688941806555, Rand Error = 0.09216045588254929\n",
      "Epoch 84: Train Loss = 0.008047713540754346, Val Loss = 0.009005443891510367, Sys Error = 0.013416491448879242, Rand Error = 0.09218572825193405\n",
      "Epoch 85: Train Loss = 0.008005741057744207, Val Loss = 0.008802260574884712, Sys Error = -0.0012737568467855453, Rand Error = 0.09208904951810837\n",
      "Epoch 86: Train Loss = 0.00792396951705044, Val Loss = 0.009535582223907112, Sys Error = 0.02583996020257473, Rand Error = 0.09239163249731064\n",
      "Epoch 87: Train Loss = 0.008019453169084912, Val Loss = 0.008794865617528558, Sys Error = 0.00041193951619789004, Rand Error = 0.09205230325460434\n",
      "Epoch 88: Train Loss = 0.007913432858321209, Val Loss = 0.008827777672559023, Sys Error = -0.005981833208352327, Rand Error = 0.09206046909093857\n",
      "Epoch 89: Train Loss = 0.008048581917835184, Val Loss = 0.008787568751722574, Sys Error = -0.00191665964666754, Rand Error = 0.09199877083301544\n",
      "Epoch 90: Train Loss = 0.007948682581204488, Val Loss = 0.009011940425261855, Sys Error = 0.014274138025939465, Rand Error = 0.09208817780017853\n",
      "Epoch 91: Train Loss = 0.007980764296642223, Val Loss = 0.008792490419000386, Sys Error = 0.0031310860067605972, Rand Error = 0.09197861701250076\n",
      "Epoch 92: Train Loss = 0.007723548909822522, Val Loss = 0.008784788008779288, Sys Error = 0.002072360832244158, Rand Error = 0.09197133034467697\n",
      "Epoch 93: Train Loss = 0.007752488302283509, Val Loss = 0.0091837618034333, Sys Error = 0.018605494871735573, Rand Error = 0.0922454372048378\n",
      "Epoch 94: Train Loss = 0.007930324033855699, Val Loss = 0.008781888475641608, Sys Error = 0.00036351411836221814, Rand Error = 0.09198909997940063\n",
      "Epoch 95: Train Loss = 0.007971892517796435, Val Loss = 0.008871229737997055, Sys Error = 0.008449952118098736, Rand Error = 0.09206572920084\n",
      "Epoch 96: Train Loss = 0.007637497384187787, Val Loss = 0.008779241470620036, Sys Error = -0.00352040259167552, Rand Error = 0.09191563725471497\n",
      "Epoch 97: Train Loss = 0.007783426377965614, Val Loss = 0.008773490041494369, Sys Error = -0.004198338836431503, Rand Error = 0.09185182303190231\n",
      "Epoch 98: Train Loss = 0.007979572829140653, Val Loss = 0.008777897199615836, Sys Error = -0.0002570149954408407, Rand Error = 0.09197025001049042\n",
      "Epoch 99: Train Loss = 0.00792116722698475, Val Loss = 0.008781409496441484, Sys Error = 0.002079793019220233, Rand Error = 0.09195657819509506\n",
      "Epoch 100: Train Loss = 0.007853388916267905, Val Loss = 0.008931102883070707, Sys Error = 0.011163304559886456, Rand Error = 0.092096246778965\n",
      "Epoch 101: Train Loss = 0.007768211507260106, Val Loss = 0.0091074263677001, Sys Error = 0.016532987356185913, Rand Error = 0.09223910421133041\n",
      "Epoch 102: Train Loss = 0.007967587565829935, Val Loss = 0.00877190325409174, Sys Error = 0.002057904377579689, Rand Error = 0.09189923852682114\n",
      "Epoch 103: Train Loss = 0.007922938743302988, Val Loss = 0.009012561244890093, Sys Error = 0.013559997081756592, Rand Error = 0.09221947193145752\n",
      "Epoch 104: Train Loss = 0.007858271829697281, Val Loss = 0.008786164037883282, Sys Error = 0.0033076382242143154, Rand Error = 0.09194274991750717\n",
      "Epoch 105: Train Loss = 0.007829986201317678, Val Loss = 0.009026027983054519, Sys Error = 0.01414383202791214, Rand Error = 0.09220073372125626\n",
      "Epoch 106: Train Loss = 0.007636077179530159, Val Loss = 0.008789604343473912, Sys Error = 0.003992444835603237, Rand Error = 0.0919312834739685\n",
      "Epoch 107: Train Loss = 0.007846257800972738, Val Loss = 0.009801899921149016, Sys Error = 0.029603522270917892, Rand Error = 0.09269438683986664\n",
      "Epoch 108: Train Loss = 0.00772590036124944, Val Loss = 0.0087803166359663, Sys Error = -0.0065860142931342125, Rand Error = 0.0917525365948677\n",
      "Epoch 109: Train Loss = 0.00784015339301076, Val Loss = 0.0087496527004987, Sys Error = 0.003776279743760824, Rand Error = 0.09169508516788483\n",
      "Epoch 110: Train Loss = 0.007889606421380195, Val Loss = 0.008734261989593506, Sys Error = -0.0029079949017614126, Rand Error = 0.09166907519102097\n",
      "Epoch 111: Train Loss = 0.007892735513620252, Val Loss = 0.008923421800136565, Sys Error = -0.014441338367760181, Rand Error = 0.09165336191654205\n",
      "Epoch 112: Train Loss = 0.007826774570614446, Val Loss = 0.008770572720095515, Sys Error = 0.0027554682455956936, Rand Error = 0.09187742322683334\n",
      "Epoch 113: Train Loss = 0.007883357842517801, Val Loss = 0.00890761222690344, Sys Error = 0.010705421678721905, Rand Error = 0.09202199429273605\n",
      "Epoch 114: Train Loss = 0.007731888248303602, Val Loss = 0.008970327116549015, Sys Error = 0.012872385792434216, Rand Error = 0.09207841008901596\n",
      "Epoch 115: Train Loss = 0.007844195582059234, Val Loss = 0.008785351784899831, Sys Error = 0.005089418962597847, Rand Error = 0.09184733778238297\n",
      "Epoch 116: Train Loss = 0.007844733675335383, Val Loss = 0.008899345993995667, Sys Error = 0.010741788893938065, Rand Error = 0.09197017550468445\n",
      "Epoch 117: Train Loss = 0.007798967032888254, Val Loss = 0.008744410611689091, Sys Error = -0.005473908502608538, Rand Error = 0.09162035584449768\n",
      "Epoch 118: Train Loss = 0.007805908504916832, Val Loss = 0.009026838280260564, Sys Error = 0.01491596270352602, Rand Error = 0.09207050502300262\n",
      "Epoch 119: Train Loss = 0.00793742129665821, Val Loss = 0.008753841510042548, Sys Error = 0.0027041430585086346, Rand Error = 0.09178462624549866\n",
      "Epoch 120: Train Loss = 0.0076960031008131286, Val Loss = 0.00891811358742416, Sys Error = 0.011152619495987892, Rand Error = 0.09202635288238525\n",
      "Epoch 121: Train Loss = 0.007590836276798401, Val Loss = 0.008724565734155476, Sys Error = 0.001108210883103311, Rand Error = 0.09165055304765701\n",
      "Epoch 122: Train Loss = 0.007847139292374947, Val Loss = 0.008769143372774124, Sys Error = 0.004441083408892155, Rand Error = 0.09179709106683731\n",
      "Epoch 123: Train Loss = 0.007781423788062881, Val Loss = 0.008919604448601603, Sys Error = 0.012258346192538738, Rand Error = 0.09187930822372437\n",
      "Epoch 124: Train Loss = 0.007833273041733476, Val Loss = 0.008855963591486216, Sys Error = 0.010358700528740883, Rand Error = 0.09176421165466309\n",
      "Epoch 125: Train Loss = 0.007732151422736256, Val Loss = 0.008705728827044368, Sys Error = -0.002518285531550646, Rand Error = 0.09152339398860931\n",
      "Epoch 126: Train Loss = 0.007866403721489532, Val Loss = 0.009187849145382643, Sys Error = 0.019325459375977516, Rand Error = 0.0921144187450409\n",
      "Epoch 127: Train Loss = 0.007900821832875006, Val Loss = 0.009305884502828122, Sys Error = 0.021756209433078766, Rand Error = 0.0922088697552681\n",
      "Epoch 128: Train Loss = 0.007829393985746213, Val Loss = 0.00871787762735039, Sys Error = 0.00023597032122779638, Rand Error = 0.09163260459899902\n",
      "Epoch 129: Train Loss = 0.007779958517139041, Val Loss = 0.008718279772438109, Sys Error = 0.00032550934702157974, Rand Error = 0.09163592010736465\n",
      "Epoch 130: Train Loss = 0.007763286478557559, Val Loss = 0.008790374267846346, Sys Error = 0.0059926630929112434, Rand Error = 0.09182976186275482\n",
      "Epoch 131: Train Loss = 0.007716754781688715, Val Loss = 0.00876284777186811, Sys Error = 0.004186707548797131, Rand Error = 0.09178370982408524\n",
      "Epoch 132: Train Loss = 0.00776067630626088, Val Loss = 0.008861367171630264, Sys Error = 0.009424583055078983, Rand Error = 0.09192120283842087\n",
      "Epoch 133: Train Loss = 0.0077613850156668315, Val Loss = 0.009056527027860284, Sys Error = 0.01607903465628624, Rand Error = 0.0920366495847702\n",
      "Epoch 134: Train Loss = 0.007755559966574574, Val Loss = 0.00881560891866684, Sys Error = 0.008351597934961319, Rand Error = 0.09176746010780334\n",
      "Epoch 135: Train Loss = 0.007697327871980189, Val Loss = 0.008726105839014054, Sys Error = -0.005614456720650196, Rand Error = 0.09152315557003021\n",
      "Epoch 136: Train Loss = 0.007739837211015266, Val Loss = 0.008796322578564286, Sys Error = 0.00660915020853281, Rand Error = 0.09182009845972061\n",
      "Epoch 137: Train Loss = 0.007854023612602505, Val Loss = 0.008708641561679543, Sys Error = -0.00016521239012945443, Rand Error = 0.09158643335103989\n",
      "Epoch 138: Train Loss = 0.007748778516881515, Val Loss = 0.008709033252671361, Sys Error = 0.002192279789596796, Rand Error = 0.09154684096574783\n",
      "Epoch 139: Train Loss = 0.0077328618594207045, Val Loss = 0.00894803199917078, Sys Error = 0.013373658061027527, Rand Error = 0.09188297390937805\n",
      "Epoch 140: Train Loss = 0.007831775047274869, Val Loss = 0.00876225926913321, Sys Error = 0.0062879459001123905, Rand Error = 0.09164159744977951\n",
      "Epoch 141: Train Loss = 0.007696186135479704, Val Loss = 0.008767216512933374, Sys Error = 0.005003444850444794, Rand Error = 0.09176880121231079\n",
      "Epoch 142: Train Loss = 0.007768491483401767, Val Loss = 0.008824887312948703, Sys Error = 0.007539087440818548, Rand Error = 0.0919092670083046\n",
      "Epoch 143: Train Loss = 0.007730263935098815, Val Loss = 0.008733564661815763, Sys Error = 0.0013469939585775137, Rand Error = 0.091727115213871\n",
      "Epoch 144: Train Loss = 0.007777943619700192, Val Loss = 0.008909866586327553, Sys Error = 0.011679532006382942, Rand Error = 0.09191833436489105\n",
      "Epoch 145: Train Loss = 0.0077604440712305, Val Loss = 0.008715586783364416, Sys Error = 0.0004935064353048801, Rand Error = 0.09163258969783783\n",
      "Epoch 146: Train Loss = 0.007739724856693038, Val Loss = 0.008904036553576588, Sys Error = 0.011251947842538357, Rand Error = 0.09194407612085342\n",
      "Epoch 147: Train Loss = 0.007570159155875444, Val Loss = 0.009380905143916606, Sys Error = 0.023085685446858406, Rand Error = 0.09228616952896118\n",
      "Epoch 148: Train Loss = 0.007736515029073628, Val Loss = 0.008748367475345731, Sys Error = 0.00582200288772583, Rand Error = 0.09159606695175171\n",
      "Epoch 149: Train Loss = 0.007723321428869006, Val Loss = 0.00873359409160912, Sys Error = 0.004902055487036705, Rand Error = 0.0915711522102356\n",
      "Epoch 150: Train Loss = 0.00757624565212186, Val Loss = 0.008829872123897076, Sys Error = 0.009048374369740486, Rand Error = 0.09178248047828674\n",
      "Epoch 151: Train Loss = 0.007771631762342051, Val Loss = 0.00870350687764585, Sys Error = 0.0011098729446530342, Rand Error = 0.09155033528804779\n",
      "Epoch 152: Train Loss = 0.007603134515933519, Val Loss = 0.008689791476354004, Sys Error = -0.0011897191870957613, Rand Error = 0.09147605299949646\n",
      "Epoch 153: Train Loss = 0.007729756510491634, Val Loss = 0.008928696997463704, Sys Error = 0.012610751204192638, Rand Error = 0.09189282357692719\n",
      "Epoch 154: Train Loss = 0.00768900460693552, Val Loss = 0.008702133176848292, Sys Error = 0.001916046836413443, Rand Error = 0.09152363240718842\n",
      "Epoch 155: Train Loss = 0.007351119049586529, Val Loss = 0.008702406799420715, Sys Error = -0.0058721983805298805, Rand Error = 0.09137308597564697\n",
      "Epoch 156: Train Loss = 0.007705630025241611, Val Loss = 0.008794574765488505, Sys Error = 0.008346724323928356, Rand Error = 0.09165074676275253\n",
      "Epoch 157: Train Loss = 0.007731935602266255, Val Loss = 0.00869276369921863, Sys Error = -0.00500838179141283, Rand Error = 0.09136851131916046\n",
      "Epoch 158: Train Loss = 0.00779425393963276, Val Loss = 0.008801723690703511, Sys Error = 0.008610823191702366, Rand Error = 0.0916665643453598\n",
      "Epoch 159: Train Loss = 0.0076779301527367776, Val Loss = 0.00880093863233924, Sys Error = -0.011944425292313099, Rand Error = 0.09134538471698761\n",
      "Epoch 160: Train Loss = 0.007758925051623305, Val Loss = 0.008696973463520408, Sys Error = -0.0007799426093697548, Rand Error = 0.09152945876121521\n",
      "Epoch 161: Train Loss = 0.0075323984167690195, Val Loss = 0.008960155490785836, Sys Error = 0.012731784954667091, Rand Error = 0.09206106513738632\n",
      "Epoch 162: Train Loss = 0.007749535966404649, Val Loss = 0.008735066512599587, Sys Error = -0.007259078323841095, Rand Error = 0.09147699177265167\n",
      "Epoch 163: Train Loss = 0.0077663994134338785, Val Loss = 0.0091443560551852, Sys Error = 0.017632439732551575, Rand Error = 0.09223144501447678\n",
      "Epoch 164: Train Loss = 0.0076865413237016565, Val Loss = 0.00877362941391766, Sys Error = 0.004661923740059137, Rand Error = 0.09183315932750702\n",
      "Epoch 165: Train Loss = 0.007716254147072864, Val Loss = 0.008724478678777814, Sys Error = 0.0015008338959887624, Rand Error = 0.09167294204235077\n",
      "Epoch 166: Train Loss = 0.007703196847551438, Val Loss = 0.008708658628165722, Sys Error = 0.000254245096584782, Rand Error = 0.09159765392541885\n",
      "Epoch 167: Train Loss = 0.007674894241486178, Val Loss = 0.008737890422344208, Sys Error = 0.0037114061415195465, Rand Error = 0.0916716679930687\n",
      "Epoch 168: Train Loss = 0.007636824297870315, Val Loss = 0.008702875045128167, Sys Error = -0.004638148006051779, Rand Error = 0.09145434200763702\n",
      "Epoch 169: Train Loss = 0.007644279521543446, Val Loss = 0.008746854122728109, Sys Error = -0.008252828381955624, Rand Error = 0.09145958721637726\n",
      "Epoch 170: Train Loss = 0.0076886342546014595, Val Loss = 0.008723113592714072, Sys Error = 0.001095270155929029, Rand Error = 0.0916752815246582\n",
      "Epoch 171: Train Loss = 0.007646204627573837, Val Loss = 0.008706149598583578, Sys Error = -0.0018786521395668387, Rand Error = 0.09157456457614899\n",
      "Epoch 172: Train Loss = 0.007587946022774072, Val Loss = 0.008706533792428672, Sys Error = -0.0022873305715620518, Rand Error = 0.09156960248947144\n",
      "Epoch 173: Train Loss = 0.00737938137053577, Val Loss = 0.008918061153963208, Sys Error = 0.01199411042034626, Rand Error = 0.09192273765802383\n",
      "Epoch 174: Train Loss = 0.007795262930178365, Val Loss = 0.008807745622470975, Sys Error = 0.0070167952217161655, Rand Error = 0.09185802936553955\n",
      "Epoch 175: Train Loss = 0.00744103396657941, Val Loss = 0.008713278593495489, Sys Error = -0.00041019724449142814, Rand Error = 0.09162937104701996\n",
      "Epoch 176: Train Loss = 0.00738559111541267, Val Loss = 0.008716127485968173, Sys Error = -0.003381685120984912, Rand Error = 0.0915956199169159\n",
      "Epoch 177: Train Loss = 0.007609936381625228, Val Loss = 0.008763822633773089, Sys Error = -0.009748414158821106, Rand Error = 0.0914074033498764\n",
      "Epoch 178: Train Loss = 0.0075271893157298824, Val Loss = 0.00875137122347951, Sys Error = 0.003980937413871288, Rand Error = 0.09173782169818878\n",
      "Epoch 179: Train Loss = 0.0076629517296719, Val Loss = 0.008722174004651606, Sys Error = -0.0026137183886021376, Rand Error = 0.09165500104427338\n",
      "Epoch 180: Train Loss = 0.007733947602858724, Val Loss = 0.009129532473161816, Sys Error = 0.0171559639275074, Rand Error = 0.09224103391170502\n",
      "Epoch 181: Train Loss = 0.00768215196250483, Val Loss = 0.00879925638437271, Sys Error = 0.00674582552164793, Rand Error = 0.09183015674352646\n",
      "Epoch 182: Train Loss = 0.007597352686706324, Val Loss = 0.008743740851059556, Sys Error = 0.003248656867071986, Rand Error = 0.09172829985618591\n",
      "Epoch 183: Train Loss = 0.007710975096669308, Val Loss = 0.008770285779610277, Sys Error = 0.0044418927282094955, Rand Error = 0.09182333946228027\n",
      "Epoch 184: Train Loss = 0.007360544467214928, Val Loss = 0.008792139450088143, Sys Error = 0.005394376814365387, Rand Error = 0.0918908417224884\n",
      "Epoch 185: Train Loss = 0.007527573694756558, Val Loss = 0.008974961284548045, Sys Error = 0.012835303321480751, Rand Error = 0.09211960434913635\n",
      "Epoch 186: Train Loss = 0.007673339512155846, Val Loss = 0.0087270587682724, Sys Error = -0.005391113925725222, Rand Error = 0.09156548231840134\n",
      "Epoch 187: Train Loss = 0.007710279967246014, Val Loss = 0.0088083874899894, Sys Error = 0.006647015921771526, Rand Error = 0.09188934415578842\n",
      "Epoch 188: Train Loss = 0.007618428332471224, Val Loss = 0.008721676655113697, Sys Error = -0.002234440064057708, Rand Error = 0.09166062623262405\n",
      "Epoch 189: Train Loss = 0.00769557325754228, Val Loss = 0.008781538670882582, Sys Error = 0.004551735706627369, Rand Error = 0.09188247472047806\n",
      "Epoch 190: Train Loss = 0.007639987708264312, Val Loss = 0.009008907712996006, Sys Error = 0.013946831226348877, Rand Error = 0.09213633835315704\n",
      "Epoch 191: Train Loss = 0.007595893547891877, Val Loss = 0.008758032927289605, Sys Error = 0.003287727478891611, Rand Error = 0.09180719405412674\n",
      "Epoch 192: Train Loss = 0.007668242269996987, Val Loss = 0.008725393889471888, Sys Error = -0.00091948063345626, Rand Error = 0.0916965901851654\n",
      "Epoch 193: Train Loss = 0.007526105968281627, Val Loss = 0.008729955041781068, Sys Error = -0.0046156845055520535, Rand Error = 0.09162217378616333\n",
      "Epoch 194: Train Loss = 0.007675736392631607, Val Loss = 0.008918731240555644, Sys Error = 0.011086822487413883, Rand Error = 0.09204310923814774\n",
      "Epoch 195: Train Loss = 0.007655871847947669, Val Loss = 0.008841542527079582, Sys Error = -0.012919493019580841, Rand Error = 0.0914626494050026\n",
      "Epoch 196: Train Loss = 0.007784712285366516, Val Loss = 0.00877325665205717, Sys Error = 0.0019769719801843166, Rand Error = 0.09193950891494751\n",
      "Epoch 197: Train Loss = 0.007675141810851042, Val Loss = 0.008817758271470666, Sys Error = 0.005181965418159962, Rand Error = 0.09204543381929398\n",
      "Epoch 198: Train Loss = 0.007634459047166761, Val Loss = 0.008811128931120038, Sys Error = -0.008125027641654015, Rand Error = 0.09184890985488892\n",
      "Epoch 199: Train Loss = 0.007614835341417685, Val Loss = 0.009025403158739209, Sys Error = 0.01318616233766079, Rand Error = 0.09234703332185745\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Sequential(\n",
       "   (layer_linear0): Linear(in_features=11, out_features=11, bias=True)\n",
       "   (layer_ReLu0): ReLU()\n",
       "   (layer_linear1): Linear(in_features=11, out_features=11, bias=True)\n",
       "   (layer_ReLu1): ReLU()\n",
       "   (last_layer): Linear(in_features=11, out_features=1, bias=True)\n",
       "   (last): Sigmoid()\n",
       " ),\n",
       " np.float32(0.09234703),\n",
       " np.float32(0.013186162))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_network(name, epochs=200, learningrate=0.1, hidden_layers=2, seed=42, optimizer='SGD', bvalues=None, batch_size=16, sims=1000):\n",
    "    \n",
    "    # Generate train, validation, and test datasets\n",
    "    trainloader, inferloader, testloader = sim_dat(bvalues, batch_size=batch_size, sims=sims)\n",
    "\n",
    "    # Initialize model\n",
    "    model = make_model(n_inputs=len(bvalues), n_hidden=hidden_layers, n_outputs=1)\n",
    "    model.apply(init_weights)  # Ensure fresh training each run\n",
    "\n",
    "    # Initialize WandB logging\n",
    "    wandb.init(project=\"AI_for_medical_imaging\", job_type=\"training\", name=name)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():  \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Set device (GPU if available, otherwise CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Define loss function\n",
    "    loss_module = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learningrate)\n",
    "    else:\n",
    "        raise NotImplementedError('This optimizer is not implemented yet...')\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        train_loss_f = 0\n",
    "        SD_train = 0\n",
    "        sys_train = 0\n",
    "\n",
    "        # Training step\n",
    "        for x in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            batch = x[0].to(device)  # Move batch to GPU\n",
    "            f_ref = torch.flatten(x[2].to(device))  # Ground truth values\n",
    "\n",
    "            # Forward pass\n",
    "            f_pred = torch.flatten(model(batch))\n",
    "\n",
    "            # Compute loss\n",
    "            loss_f = loss_module(f_pred, f_ref)\n",
    "            train_loss_f += loss_f.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss_f.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute training error metrics\n",
    "            SD, sys = hf.error_metrics(f_pred.cpu().detach().numpy(), f_ref.cpu().detach().numpy())\n",
    "            SD_train += SD**2\n",
    "            sys_train += sys\n",
    "\n",
    "        # Calculate mean training errors\n",
    "        SD_train = np.sqrt(SD_train / trainloader.__len__())\n",
    "        sys_train = sys_train / trainloader.__len__()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss_f = 0\n",
    "        SD_val = 0\n",
    "        sys_val = 0\n",
    "\n",
    "        all_f_ref = []\n",
    "        all_f_pred = []\n",
    "\n",
    "        with torch.no_grad():  # No gradients during validation\n",
    "            for x in inferloader:\n",
    "                batch = x[0].to(device)\n",
    "                f_ref = torch.flatten(x[2].to(device))\n",
    "\n",
    "                # Forward pass\n",
    "                f_pred = torch.flatten(model(batch))\n",
    "\n",
    "                # Compute validation loss\n",
    "                loss_f = loss_module(f_pred, f_ref)\n",
    "                val_loss_f += loss_f.item()\n",
    "\n",
    "                # Compute validation error metrics\n",
    "                SD, sys = hf.error_metrics(f_pred.cpu().numpy(), f_ref.cpu().numpy())\n",
    "                SD_val += SD**2\n",
    "                sys_val += sys\n",
    "\n",
    "                # Collect predictions for logging\n",
    "                all_f_ref.extend(f_ref.cpu().detach().numpy())  \n",
    "                all_f_pred.extend(f_pred.cpu().detach().numpy())\n",
    "\n",
    "        # Calculate mean validation errors\n",
    "        SD_val = np.sqrt(SD_val / inferloader.__len__())\n",
    "        sys_val = sys_val / inferloader.__len__()\n",
    "\n",
    "        # Logging to WandB every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            example_data = [[x, y] for (x, y) in zip(all_f_ref, all_f_pred)]\n",
    "            table = wandb.Table(data=example_data, columns=[\"f_ref\", \"f_pred\"])\n",
    "\n",
    "            wandb.log({\n",
    "                \"loss/train\": train_loss_f / trainloader.__len__(),\n",
    "                \"loss/val\": val_loss_f / inferloader.__len__(),\n",
    "                \"error/random error\": SD_train,\n",
    "                \"error/systematic error\": sys_train,\n",
    "                \"data_plot epoch \" + str(epoch): wandb.plot.scatter(table, \"f_ref\", \"f_pred\", title=f'epoch{epoch}')\n",
    "            })\n",
    "\n",
    "\n",
    "        # Print output in terminal (optional for debugging)\n",
    "        print(f'Epoch {epoch}: Train Loss = {train_loss_f / trainloader.__len__()}, '\n",
    "              f'Val Loss = {val_loss_f / inferloader.__len__()}, '\n",
    "              f'Sys Error = {sys_val}, Rand Error = {SD_val}')\n",
    "\n",
    "    # Finish WandB logging\n",
    "    wandb.finish()\n",
    "\n",
    "    return model, SD_val, sys_val \n",
    "\n",
    "bvalues = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # Example values\n",
    "train_network('test', bvalues=bvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 0.04784128160844015, Val Loss = 0.031609976664185524, Sys Error = 0.01419585756957531, Rand Error = 0.1697169989347458\n",
      "Epoch 1: Train Loss = 0.030208487150280974, Val Loss = 0.0291206244379282, Sys Error = -0.003593146800994873, Rand Error = 0.1633608192205429\n",
      "Epoch 2: Train Loss = 0.028559261803017107, Val Loss = 0.027487883903086187, Sys Error = -0.004485107958316803, Rand Error = 0.15869049727916718\n",
      "Epoch 3: Train Loss = 0.026978224559232246, Val Loss = 0.025983766466379166, Sys Error = 0.002572158817201853, Rand Error = 0.15433968603610992\n",
      "Epoch 4: Train Loss = 0.02526748643884825, Val Loss = 0.02418445311486721, Sys Error = -0.0008901081746444106, Rand Error = 0.1489163637161255\n",
      "Epoch 5: Train Loss = 0.023737444737276365, Val Loss = 0.02235581036657095, Sys Error = -0.0035729892551898956, Rand Error = 0.1431407481431961\n",
      "Epoch 6: Train Loss = 0.021789606097479198, Val Loss = 0.020966020040214063, Sys Error = 0.01261572539806366, Rand Error = 0.13814517855644226\n",
      "Epoch 7: Train Loss = 0.02021563434323599, Val Loss = 0.01897328207269311, Sys Error = 0.008101697079837322, Rand Error = 0.13172046840190887\n",
      "Epoch 8: Train Loss = 0.01824992388313593, Val Loss = 0.017090342845767735, Sys Error = -0.002292861696332693, Rand Error = 0.12525583803653717\n",
      "Epoch 9: Train Loss = 0.01666052991437704, Val Loss = 0.01570785902440548, Sys Error = 0.014122587628662586, Rand Error = 0.11940331012010574\n",
      "Epoch 10: Train Loss = 0.015205662849164286, Val Loss = 0.014239265862852336, Sys Error = 0.01561467070132494, Rand Error = 0.11351238191127777\n",
      "Epoch 11: Train Loss = 0.013788729949399482, Val Loss = 0.013202629052102566, Sys Error = 0.02178432047367096, Rand Error = 0.10834742337465286\n",
      "Epoch 12: Train Loss = 0.01270459947544475, Val Loss = 0.011714798398315907, Sys Error = 0.009329073131084442, Rand Error = 0.10369475930929184\n",
      "Epoch 13: Train Loss = 0.011716936812411214, Val Loss = 0.011109990347176791, Sys Error = 0.019213074818253517, Rand Error = 0.09978833049535751\n",
      "Epoch 14: Train Loss = 0.010959094790958388, Val Loss = 0.010331290168687701, Sys Error = 0.01632734201848507, Rand Error = 0.09673682600259781\n",
      "Epoch 15: Train Loss = 0.010243389230264827, Val Loss = 0.010080472007393837, Sys Error = 0.02358691394329071, Rand Error = 0.09422831237316132\n",
      "Epoch 16: Train Loss = 0.009772663123819024, Val Loss = 0.00937471929937601, Sys Error = 0.013419714756309986, Rand Error = 0.09269575774669647\n",
      "Epoch 17: Train Loss = 0.009305771960075511, Val Loss = 0.009192955191247165, Sys Error = 0.01728210784494877, Rand Error = 0.0912863165140152\n",
      "Epoch 18: Train Loss = 0.009326539754997505, Val Loss = 0.008879999071359635, Sys Error = 0.012141196057200432, Rand Error = 0.09053559601306915\n",
      "Epoch 19: Train Loss = 0.009157304137641953, Val Loss = 0.008909173915162683, Sys Error = 0.019238894805312157, Rand Error = 0.08962668478488922\n",
      "Epoch 20: Train Loss = 0.008917395458665005, Val Loss = 0.008896118123084306, Sys Error = 0.02175666019320488, Rand Error = 0.08908284455537796\n",
      "Epoch 21: Train Loss = 0.00874227874500807, Val Loss = 0.008896499592810869, Sys Error = 0.023723188787698746, Rand Error = 0.08866695314645767\n",
      "Epoch 22: Train Loss = 0.008773080836764948, Val Loss = 0.008613608498126268, Sys Error = 0.017259668558835983, Rand Error = 0.08861275017261505\n",
      "Epoch 23: Train Loss = 0.008728746955586208, Val Loss = 0.008445042488165199, Sys Error = 0.011208015494048595, Rand Error = 0.08865892887115479\n",
      "Epoch 24: Train Loss = 0.008624105004940269, Val Loss = 0.008707780949771404, Sys Error = 0.022338729351758957, Rand Error = 0.08812908828258514\n",
      "Epoch 25: Train Loss = 0.008593422003350285, Val Loss = 0.008540455298498274, Sys Error = 0.01826036162674427, Rand Error = 0.08814281225204468\n",
      "Epoch 26: Train Loss = 0.008581566960052696, Val Loss = 0.008324924041517079, Sys Error = 0.00013948604464530945, Rand Error = 0.0887497216463089\n",
      "Epoch 27: Train Loss = 0.00866794801956086, Val Loss = 0.008543727407231926, Sys Error = 0.019540341570973396, Rand Error = 0.08794885873794556\n",
      "Epoch 28: Train Loss = 0.008467050854030043, Val Loss = 0.008316519926302135, Sys Error = 0.010615934617817402, Rand Error = 0.08818129450082779\n",
      "Epoch 29: Train Loss = 0.008442829682513378, Val Loss = 0.008255480346269906, Sys Error = 0.005145506002008915, Rand Error = 0.0883200541138649\n",
      "Epoch 30: Train Loss = 0.008163500923750013, Val Loss = 0.008728735847398639, Sys Error = 0.02522015944123268, Rand Error = 0.08765571564435959\n",
      "Epoch 31: Train Loss = 0.008411386694629179, Val Loss = 0.008596058306284248, Sys Error = 0.022426679730415344, Rand Error = 0.08766720443964005\n",
      "Epoch 32: Train Loss = 0.008335462029546846, Val Loss = 0.008503692946396768, Sys Error = 0.0203382670879364, Rand Error = 0.08766462653875351\n",
      "Epoch 33: Train Loss = 0.008470030745144847, Val Loss = 0.008724823663942515, Sys Error = 0.02600662037730217, Rand Error = 0.08745867758989334\n",
      "Epoch 34: Train Loss = 0.008513921503584052, Val Loss = 0.008242495986633003, Sys Error = 0.011621036566793919, Rand Error = 0.08776941150426865\n",
      "Epoch 35: Train Loss = 0.008398141476412326, Val Loss = 0.008162141195498407, Sys Error = 0.0044925762340426445, Rand Error = 0.08794887363910675\n",
      "Epoch 36: Train Loss = 0.00849053215473717, Val Loss = 0.00815004319883883, Sys Error = 0.005502917803823948, Rand Error = 0.08785070478916168\n",
      "Epoch 37: Train Loss = 0.00834525162949725, Val Loss = 0.008316967240534722, Sys Error = 0.016592714935541153, Rand Error = 0.08746165782213211\n",
      "Epoch 38: Train Loss = 0.008356682523045429, Val Loss = 0.008432292309589685, Sys Error = 0.020478926599025726, Rand Error = 0.08732211589813232\n",
      "Epoch 39: Train Loss = 0.008306050510687191, Val Loss = 0.00812302976846695, Sys Error = 0.007343473844230175, Rand Error = 0.08761580288410187\n",
      "Epoch 40: Train Loss = 0.008384898153328619, Val Loss = 0.00810534085612744, Sys Error = 0.00679826270788908, Rand Error = 0.08756861090660095\n",
      "Epoch 41: Train Loss = 0.00838328106328845, Val Loss = 0.00817248336970806, Sys Error = 0.012666190043091774, Rand Error = 0.08733364194631577\n",
      "Epoch 42: Train Loss = 0.008326570844537643, Val Loss = 0.008227773732505739, Sys Error = 0.015620706602931023, Rand Error = 0.08718875050544739\n",
      "Epoch 43: Train Loss = 0.008290417427413686, Val Loss = 0.008296538051217795, Sys Error = 0.0183306522667408, Rand Error = 0.08707364648580551\n",
      "Epoch 44: Train Loss = 0.008315270782859867, Val Loss = 0.00833379067480564, Sys Error = 0.01982210949063301, Rand Error = 0.08697393536567688\n",
      "Epoch 45: Train Loss = 0.008239367316195439, Val Loss = 0.00818291816394776, Sys Error = 0.0153036592528224, Rand Error = 0.08702416718006134\n",
      "Epoch 46: Train Loss = 0.008237156015279334, Val Loss = 0.008122568018734455, Sys Error = 0.013029001653194427, Rand Error = 0.0870606005191803\n",
      "Epoch 47: Train Loss = 0.00804450852948046, Val Loss = 0.008180523826740681, Sys Error = 0.0157896988093853, Rand Error = 0.08695593476295471\n",
      "Epoch 48: Train Loss = 0.008025818293221122, Val Loss = 0.008032317901961506, Sys Error = 0.008740808814764023, Rand Error = 0.08709201216697693\n",
      "Epoch 49: Train Loss = 0.008237671805545688, Val Loss = 0.008881029998883605, Sys Error = 0.03167615085840225, Rand Error = 0.0866934135556221\n",
      "Epoch 50: Train Loss = 0.00823370358083657, Val Loss = 0.008258901955559849, Sys Error = 0.018917478621006012, Rand Error = 0.08681686222553253\n",
      "Epoch 51: Train Loss = 0.008154742565915682, Val Loss = 0.008952310821041465, Sys Error = 0.032966192811727524, Rand Error = 0.08665603399276733\n",
      "Epoch 52: Train Loss = 0.00820821534495714, Val Loss = 0.008066559513099491, Sys Error = 0.01244189590215683, Rand Error = 0.08688681572675705\n",
      "Epoch 53: Train Loss = 0.008087703086392478, Val Loss = 0.007980027422308921, Sys Error = -0.001890763291157782, Rand Error = 0.0872112289071083\n",
      "Epoch 54: Train Loss = 0.008057792029386862, Val Loss = 0.007993661821819842, Sys Error = 0.008969319984316826, Rand Error = 0.08690519630908966\n",
      "Epoch 55: Train Loss = 0.008158447924828114, Val Loss = 0.008053523185662926, Sys Error = 0.012629044242203236, Rand Error = 0.08680490404367447\n",
      "Epoch 56: Train Loss = 0.008178695622116847, Val Loss = 0.008469629567116499, Sys Error = 0.024645429104566574, Rand Error = 0.08665642887353897\n",
      "Epoch 57: Train Loss = 0.007736854352663422, Val Loss = 0.008791762217879296, Sys Error = 0.03068559803068638, Rand Error = 0.08660732954740524\n",
      "Epoch 58: Train Loss = 0.007775834236337349, Val Loss = 0.008012918569147588, Sys Error = 0.011261011473834515, Rand Error = 0.0867907702922821\n",
      "Epoch 59: Train Loss = 0.008131175107041071, Val Loss = 0.007933372817933559, Sys Error = -0.0007771843811497092, Rand Error = 0.08701682090759277\n",
      "Epoch 60: Train Loss = 0.007981210053616832, Val Loss = 0.008435022179037332, Sys Error = 0.024243291467428207, Rand Error = 0.08659900724887848\n",
      "Epoch 61: Train Loss = 0.007890325878898418, Val Loss = 0.007955948752351106, Sys Error = 0.008344052359461784, Rand Error = 0.08681011199951172\n",
      "Epoch 62: Train Loss = 0.00808962294019672, Val Loss = 0.007995054125785828, Sys Error = 0.011006148532032967, Rand Error = 0.08676102012395859\n",
      "Epoch 63: Train Loss = 0.008063237204454666, Val Loss = 0.00791027711238712, Sys Error = 0.002992960624396801, Rand Error = 0.08689429610967636\n",
      "Epoch 64: Train Loss = 0.00803890354324912, Val Loss = 0.00793999673333019, Sys Error = 0.008059768006205559, Rand Error = 0.08677007257938385\n",
      "Epoch 65: Train Loss = 0.007965793908941884, Val Loss = 0.00800950441043824, Sys Error = 0.01237698458135128, Rand Error = 0.08668576925992966\n",
      "Epoch 66: Train Loss = 0.007925571245682795, Val Loss = 0.00792771321721375, Sys Error = 0.007743542082607746, Rand Error = 0.08673782646656036\n",
      "Epoch 67: Train Loss = 0.007994353013156458, Val Loss = 0.008019252168014645, Sys Error = 0.013197669759392738, Rand Error = 0.08662573248147964\n",
      "Epoch 68: Train Loss = 0.00802675855540952, Val Loss = 0.008220608322881162, Sys Error = 0.019739840179681778, Rand Error = 0.08656423538923264\n",
      "Epoch 69: Train Loss = 0.008012080223970982, Val Loss = 0.008204209129326045, Sys Error = 0.019415298476815224, Rand Error = 0.08655752241611481\n",
      "Epoch 70: Train Loss = 0.008053922365138005, Val Loss = 0.008387818606570363, Sys Error = 0.02380620874464512, Rand Error = 0.08653849363327026\n",
      "Epoch 71: Train Loss = 0.007561633292975468, Val Loss = 0.008698076172731817, Sys Error = 0.029740747064352036, Rand Error = 0.08652985841035843\n",
      "Epoch 72: Train Loss = 0.007945004612380681, Val Loss = 0.008065412240102886, Sys Error = 0.015369601547718048, Rand Error = 0.08659877628087997\n",
      "Epoch 73: Train Loss = 0.00801562347048677, Val Loss = 0.007970448536798357, Sys Error = 0.011771162040531635, Rand Error = 0.086600661277771\n",
      "Epoch 74: Train Loss = 0.007967811126565172, Val Loss = 0.007872211909852923, Sys Error = 0.0038413251750171185, Rand Error = 0.08672759681940079\n",
      "Epoch 75: Train Loss = 0.008011828469069199, Val Loss = 0.007926204334944486, Sys Error = 0.009460939094424248, Rand Error = 0.08664195239543915\n",
      "Epoch 76: Train Loss = 0.007956641959026456, Val Loss = 0.007922513294033706, Sys Error = 0.009359432384371758, Rand Error = 0.08663719892501831\n",
      "Epoch 77: Train Loss = 0.00800364687876386, Val Loss = 0.007868616096675395, Sys Error = 0.0023950699251145124, Rand Error = 0.08678334951400757\n",
      "Epoch 78: Train Loss = 0.0079549336751793, Val Loss = 0.00795006682164967, Sys Error = 0.0110485153272748, Rand Error = 0.08662422001361847\n",
      "Epoch 79: Train Loss = 0.00799284365872831, Val Loss = 0.007861931133084, Sys Error = 0.002080372301861644, Rand Error = 0.08676327764987946\n",
      "Epoch 80: Train Loss = 0.007863704304665674, Val Loss = 0.007929690089076758, Sys Error = 0.010252738371491432, Rand Error = 0.08660976588726044\n",
      "Epoch 81: Train Loss = 0.007971108880240557, Val Loss = 0.007858473877422511, Sys Error = 0.0014488536398857832, Rand Error = 0.08676253259181976\n",
      "Epoch 82: Train Loss = 0.008015914497962006, Val Loss = 0.00814641360193491, Sys Error = 0.018609629943966866, Rand Error = 0.08649888634681702\n",
      "Epoch 83: Train Loss = 0.007850915772927016, Val Loss = 0.007920684362761676, Sys Error = 0.00999535247683525, Rand Error = 0.08660359680652618\n",
      "Epoch 84: Train Loss = 0.007542029001511807, Val Loss = 0.007957673515193164, Sys Error = 0.012085257098078728, Rand Error = 0.08656437695026398\n",
      "Epoch 85: Train Loss = 0.007826298004189549, Val Loss = 0.008001828962005676, Sys Error = -0.009933536872267723, Rand Error = 0.08700790256261826\n",
      "Epoch 86: Train Loss = 0.007910222004622568, Val Loss = 0.007889643288217485, Sys Error = -0.003714030608534813, Rand Error = 0.08689029514789581\n",
      "Epoch 87: Train Loss = 0.007925138914372859, Val Loss = 0.008234661282040178, Sys Error = 0.02111632190644741, Rand Error = 0.08648523688316345\n",
      "Epoch 88: Train Loss = 0.007966328877955675, Val Loss = 0.00787715008482337, Sys Error = 0.007532595656812191, Rand Error = 0.08662067353725433\n",
      "Epoch 89: Train Loss = 0.00797373349345181, Val Loss = 0.00807700699660927, Sys Error = 0.016886305063962936, Rand Error = 0.08648813515901566\n",
      "Epoch 90: Train Loss = 0.007902934670794842, Val Loss = 0.007903328770771623, Sys Error = 0.009898859076201916, Rand Error = 0.08653999119997025\n",
      "Epoch 91: Train Loss = 0.007892359589594741, Val Loss = 0.008540720888413489, Sys Error = 0.02768026851117611, Rand Error = 0.08644397556781769\n",
      "Epoch 92: Train Loss = 0.007902793228886155, Val Loss = 0.008090104861184954, Sys Error = 0.017466144636273384, Rand Error = 0.08647357672452927\n",
      "Epoch 93: Train Loss = 0.007810101554685727, Val Loss = 0.007895804406143726, Sys Error = 0.009554819203913212, Rand Error = 0.08656194806098938\n",
      "Epoch 94: Train Loss = 0.00769951855114033, Val Loss = 0.008232245407998561, Sys Error = 0.021390678361058235, Rand Error = 0.08643199503421783\n",
      "Epoch 95: Train Loss = 0.007940946834508416, Val Loss = 0.007842471636831761, Sys Error = 0.004703315906226635, Rand Error = 0.08664396405220032\n",
      "Epoch 96: Train Loss = 0.007959819290527078, Val Loss = 0.007943158294074238, Sys Error = 0.01233895868062973, Rand Error = 0.08649913966655731\n",
      "Epoch 97: Train Loss = 0.007939436922369654, Val Loss = 0.008050829404965043, Sys Error = 0.016429055482149124, Rand Error = 0.08646625280380249\n",
      "Epoch 98: Train Loss = 0.007791940355673432, Val Loss = 0.007833521766588091, Sys Error = 0.003694733139127493, Rand Error = 0.0866517648100853\n",
      "Epoch 99: Train Loss = 0.007852057504012834, Val Loss = 0.007905309903435409, Sys Error = 0.010665812529623508, Rand Error = 0.08651376515626907\n",
      "Epoch 100: Train Loss = 0.007810601607224969, Val Loss = 0.008251713472418487, Sys Error = 0.022018473595380783, Rand Error = 0.08641860634088516\n",
      "Epoch 101: Train Loss = 0.00790570407217845, Val Loss = 0.007834118558093905, Sys Error = 0.0053012981079518795, Rand Error = 0.08658166974782944\n",
      "Epoch 102: Train Loss = 0.007823408814147115, Val Loss = 0.008095415635034442, Sys Error = 0.018050601705908775, Rand Error = 0.08642124384641647\n",
      "Epoch 103: Train Loss = 0.00785539447611501, Val Loss = 0.007939607067964972, Sys Error = 0.012635025195777416, Rand Error = 0.08646555989980698\n",
      "Epoch 104: Train Loss = 0.007907033367299064, Val Loss = 0.007817254750989377, Sys Error = 0.0022884581703692675, Rand Error = 0.08661321550607681\n",
      "Epoch 105: Train Loss = 0.00782969962199067, Val Loss = 0.008291756897233427, Sys Error = 0.023091228678822517, Rand Error = 0.0863916203379631\n",
      "Epoch 106: Train Loss = 0.0077402457096722235, Val Loss = 0.007822400610893964, Sys Error = 0.0039240955375134945, Rand Error = 0.08660702407360077\n",
      "Epoch 107: Train Loss = 0.007549681278422128, Val Loss = 0.007979509793221951, Sys Error = 0.014200468547642231, Rand Error = 0.08649004995822906\n",
      "Epoch 108: Train Loss = 0.007814430875357154, Val Loss = 0.007928631594404578, Sys Error = 0.012309429235756397, Rand Error = 0.08647263795137405\n",
      "Epoch 109: Train Loss = 0.007765833098917853, Val Loss = 0.00800857851281762, Sys Error = 0.015553658828139305, Rand Error = 0.08642685413360596\n",
      "Epoch 110: Train Loss = 0.007883126679591314, Val Loss = 0.007819440355524421, Sys Error = 0.004026072099804878, Rand Error = 0.08660182356834412\n",
      "Epoch 111: Train Loss = 0.007683194651225105, Val Loss = 0.00785308328922838, Sys Error = 0.008228528313338757, Rand Error = 0.08652079850435257\n",
      "Epoch 112: Train Loss = 0.007895267075794033, Val Loss = 0.00798975268844515, Sys Error = 0.01512173842638731, Rand Error = 0.08639777451753616\n",
      "Epoch 113: Train Loss = 0.007862435453463086, Val Loss = 0.00782204894348979, Sys Error = 0.006219944916665554, Rand Error = 0.08648363500833511\n",
      "Epoch 114: Train Loss = 0.007810839132384159, Val Loss = 0.00784232837613672, Sys Error = 0.007892133668065071, Rand Error = 0.08649193495512009\n",
      "Epoch 115: Train Loss = 0.007757199890261819, Val Loss = 0.007805868238210678, Sys Error = 0.0015239858767017722, Rand Error = 0.08660846948623657\n",
      "Epoch 116: Train Loss = 0.007826881564417205, Val Loss = 0.007820996711961925, Sys Error = 0.006208427716046572, Rand Error = 0.08650065213441849\n",
      "Epoch 117: Train Loss = 0.007672188015719659, Val Loss = 0.007810742687433958, Sys Error = 0.0021496391855180264, Rand Error = 0.08663895726203918\n",
      "Epoch 118: Train Loss = 0.007840290909286502, Val Loss = 0.007823780388571322, Sys Error = 0.006536881439387798, Rand Error = 0.08650553226470947\n",
      "Epoch 119: Train Loss = 0.007879788050601301, Val Loss = 0.008087507635354995, Sys Error = 0.018310897052288055, Rand Error = 0.0863892212510109\n",
      "Epoch 120: Train Loss = 0.007813180141627442, Val Loss = 0.008025898155756295, Sys Error = 0.016493534669280052, Rand Error = 0.08639232814311981\n",
      "Epoch 121: Train Loss = 0.00789307475328272, Val Loss = 0.007913831528276205, Sys Error = 0.012385984882712364, Rand Error = 0.08641087263822556\n",
      "Epoch 122: Train Loss = 0.007819173917131023, Val Loss = 0.00780121018178761, Sys Error = 0.0014579446287825704, Rand Error = 0.08660132437944412\n",
      "Epoch 123: Train Loss = 0.007604612633152757, Val Loss = 0.008006852259859443, Sys Error = 0.01573013886809349, Rand Error = 0.08644092828035355\n",
      "Epoch 124: Train Loss = 0.007837592093490584, Val Loss = 0.008714463049545883, Sys Error = 0.03119983710348606, Rand Error = 0.08637982606887817\n",
      "Epoch 125: Train Loss = 0.007943818058688627, Val Loss = 0.007816416816785932, Sys Error = 0.006170552223920822, Rand Error = 0.08650299161672592\n",
      "Epoch 126: Train Loss = 0.007810786978319981, Val Loss = 0.007810628856532276, Sys Error = 0.005359739996492863, Rand Error = 0.0865257978439331\n",
      "Epoch 127: Train Loss = 0.0076700949738192, Val Loss = 0.008367751212790608, Sys Error = 0.025039803236722946, Rand Error = 0.0863770991563797\n",
      "Epoch 128: Train Loss = 0.00781819634247831, Val Loss = 0.00785742502193898, Sys Error = 0.00943928211927414, Rand Error = 0.08646999299526215\n",
      "Epoch 129: Train Loss = 0.007884350139647722, Val Loss = 0.007870023790746927, Sys Error = 0.010108144953846931, Rand Error = 0.08647379279136658\n",
      "Epoch 130: Train Loss = 0.007676905113120758, Val Loss = 0.007960283732973038, Sys Error = 0.013875581324100494, Rand Error = 0.0865035355091095\n",
      "Epoch 131: Train Loss = 0.007837789950805694, Val Loss = 0.007824277342297136, Sys Error = 0.0063349963165819645, Rand Error = 0.08656056225299835\n",
      "Epoch 132: Train Loss = 0.007584624003272417, Val Loss = 0.007881887559778988, Sys Error = -0.0060234395787119865, Rand Error = 0.08687932789325714\n",
      "Epoch 133: Train Loss = 0.007804773942848971, Val Loss = 0.007912840275093914, Sys Error = 0.01225257944315672, Rand Error = 0.08646243065595627\n",
      "Epoch 134: Train Loss = 0.00783664845255052, Val Loss = 0.007803877047263086, Sys Error = 0.005527873057872057, Rand Error = 0.086479552090168\n",
      "Epoch 135: Train Loss = 0.007823005031664358, Val Loss = 0.00787739169318229, Sys Error = 0.010877284221351147, Rand Error = 0.08642707020044327\n",
      "Epoch 136: Train Loss = 0.007832486198695247, Val Loss = 0.007847094093449414, Sys Error = 0.009048384614288807, Rand Error = 0.08645999431610107\n",
      "Epoch 137: Train Loss = 0.007757918527458123, Val Loss = 0.007812189846299588, Sys Error = 0.006388739682734013, Rand Error = 0.08648641407489777\n",
      "Epoch 138: Train Loss = 0.00775128653879429, Val Loss = 0.007856523827649653, Sys Error = 0.00958851259201765, Rand Error = 0.08646722882986069\n",
      "Epoch 139: Train Loss = 0.0077307906900640835, Val Loss = 0.007819998473860324, Sys Error = -0.002237805863842368, Rand Error = 0.08672276884317398\n",
      "Epoch 140: Train Loss = 0.0075706762185883385, Val Loss = 0.007865816494449973, Sys Error = 0.009841935709118843, Rand Error = 0.0865052193403244\n",
      "Epoch 141: Train Loss = 0.007756361932775309, Val Loss = 0.008297806279733777, Sys Error = 0.023687263950705528, Rand Error = 0.08636292815208435\n",
      "Epoch 142: Train Loss = 0.007591412153614815, Val Loss = 0.0078000413253903386, Sys Error = 0.003524028230458498, Rand Error = 0.08658680319786072\n",
      "Epoch 143: Train Loss = 0.007759806878734813, Val Loss = 0.008680479787290096, Sys Error = 0.030731331557035446, Rand Error = 0.08639123290777206\n",
      "Epoch 144: Train Loss = 0.007640946093340253, Val Loss = 0.007816066476516425, Sys Error = 0.006648513488471508, Rand Error = 0.08650755137205124\n",
      "Epoch 145: Train Loss = 0.007775435029247472, Val Loss = 0.007924138591624797, Sys Error = 0.012968795374035835, Rand Error = 0.08644367754459381\n",
      "Epoch 146: Train Loss = 0.007657407723410532, Val Loss = 0.007805567048490047, Sys Error = 0.004936583805829287, Rand Error = 0.08656458556652069\n",
      "Epoch 147: Train Loss = 0.007840546302843926, Val Loss = 0.007849818957038224, Sys Error = 0.009190873242914677, Rand Error = 0.08648921549320221\n",
      "Epoch 148: Train Loss = 0.007480859025489799, Val Loss = 0.007928078598342835, Sys Error = 0.013027275912463665, Rand Error = 0.08647067844867706\n",
      "Epoch 149: Train Loss = 0.007777429933031631, Val Loss = 0.008514746930450201, Sys Error = 0.027902524918317795, Rand Error = 0.08639687299728394\n",
      "Epoch 150: Train Loss = 0.007799090563081378, Val Loss = 0.007889688387513161, Sys Error = 0.011383427307009697, Rand Error = 0.08647262305021286\n",
      "Epoch 151: Train Loss = 0.007690874004173417, Val Loss = 0.00789405673276633, Sys Error = 0.011585153639316559, Rand Error = 0.08647508919239044\n",
      "Epoch 152: Train Loss = 0.00776937359749058, Val Loss = 0.007801397331058979, Sys Error = 0.00317977974191308, Rand Error = 0.0866333544254303\n",
      "Epoch 153: Train Loss = 0.007718733912550433, Val Loss = 0.007824141159653663, Sys Error = -0.0032057114876806736, Rand Error = 0.08673622459173203\n",
      "Epoch 154: Train Loss = 0.007680793883019062, Val Loss = 0.008082893677055836, Sys Error = 0.01842980459332466, Rand Error = 0.08641756325960159\n",
      "Epoch 155: Train Loss = 0.0077837377970735, Val Loss = 0.007883103261701763, Sys Error = 0.011284795589745045, Rand Error = 0.08645627647638321\n",
      "Epoch 156: Train Loss = 0.007792218263412631, Val Loss = 0.008070083754137158, Sys Error = 0.01812635362148285, Rand Error = 0.0864013209939003\n",
      "Epoch 157: Train Loss = 0.007501325981561528, Val Loss = 0.007898950134404004, Sys Error = 0.011907937005162239, Rand Error = 0.08647266030311584\n",
      "Epoch 158: Train Loss = 0.007724898455794467, Val Loss = 0.007868270529434085, Sys Error = 0.010003158822655678, Rand Error = 0.08653592318296432\n",
      "Epoch 159: Train Loss = 0.007659212052605526, Val Loss = 0.007802689913660288, Sys Error = 0.0041113789193332195, Rand Error = 0.08661540597677231\n",
      "Epoch 160: Train Loss = 0.007750600424790105, Val Loss = 0.007931907754391431, Sys Error = 0.013517120853066444, Rand Error = 0.0864366739988327\n",
      "Epoch 161: Train Loss = 0.007579029001685423, Val Loss = 0.008110079797916114, Sys Error = 0.019143957644701004, Rand Error = 0.08643347769975662\n",
      "Epoch 162: Train Loss = 0.007767224508380994, Val Loss = 0.008142472384497524, Sys Error = 0.02011047676205635, Rand Error = 0.08639977127313614\n",
      "Epoch 163: Train Loss = 0.007807904139680918, Val Loss = 0.007890360313467681, Sys Error = 0.01165472436696291, Rand Error = 0.08645898848772049\n",
      "Epoch 164: Train Loss = 0.007803430461303093, Val Loss = 0.008325792849063873, Sys Error = 0.024338113144040108, Rand Error = 0.08639152348041534\n",
      "Epoch 165: Train Loss = 0.007748977974230467, Val Loss = 0.007809573644772172, Sys Error = 0.005761726759374142, Rand Error = 0.08656598627567291\n",
      "Epoch 166: Train Loss = 0.0077662799983870155, Val Loss = 0.007823070045560598, Sys Error = 0.0070079355500638485, Rand Error = 0.0865619108080864\n",
      "Epoch 167: Train Loss = 0.0077339523461062544, Val Loss = 0.007899914705194532, Sys Error = 0.012321488931775093, Rand Error = 0.08642695099115372\n",
      "Epoch 168: Train Loss = 0.007737820572761255, Val Loss = 0.0077947872458025815, Sys Error = 0.004571003373712301, Rand Error = 0.08654622733592987\n",
      "Epoch 169: Train Loss = 0.007705760505158714, Val Loss = 0.007837435719557107, Sys Error = 0.008551506325602531, Rand Error = 0.0865134745836258\n",
      "Epoch 170: Train Loss = 0.007822236468538989, Val Loss = 0.007795069110579788, Sys Error = 0.0018965012859553099, Rand Error = 0.08664499968290329\n",
      "Epoch 171: Train Loss = 0.007749359859803388, Val Loss = 0.007918883324600756, Sys Error = 0.012750325724482536, Rand Error = 0.08648808300495148\n",
      "Epoch 172: Train Loss = 0.007751458773909267, Val Loss = 0.007802762580104173, Sys Error = -0.0007871572161093354, Rand Error = 0.08669587224721909\n",
      "Epoch 173: Train Loss = 0.007722274227024511, Val Loss = 0.007809630385600031, Sys Error = 0.0060224891640245914, Rand Error = 0.08655541390180588\n",
      "Epoch 174: Train Loss = 0.007803246847267241, Val Loss = 0.00779936604667455, Sys Error = 0.0001533081813249737, Rand Error = 0.08668451756238937\n",
      "Epoch 175: Train Loss = 0.007679335594307198, Val Loss = 0.007945330371148885, Sys Error = 0.01408512145280838, Rand Error = 0.08643268048763275\n",
      "Epoch 176: Train Loss = 0.007778449774568164, Val Loss = 0.0077880060533061625, Sys Error = 0.0006427096086554229, Rand Error = 0.08660968393087387\n",
      "Epoch 177: Train Loss = 0.0077685214156761415, Val Loss = 0.007834257697686553, Sys Error = 0.007973523810505867, Rand Error = 0.08655479550361633\n",
      "Epoch 178: Train Loss = 0.007440893548042621, Val Loss = 0.007799078547395766, Sys Error = 0.0011072892230004072, Rand Error = 0.08668576925992966\n",
      "Epoch 179: Train Loss = 0.0077687483010163835, Val Loss = 0.007819431344978511, Sys Error = 0.005884561687707901, Rand Error = 0.08663612604141235\n",
      "Epoch 180: Train Loss = 0.007714816352872308, Val Loss = 0.008215641300193966, Sys Error = 0.021831229329109192, Rand Error = 0.08642274141311646\n",
      "Epoch 181: Train Loss = 0.007775651247695435, Val Loss = 0.007836160645820201, Sys Error = 0.007845031097531319, Rand Error = 0.08658336848020554\n",
      "Epoch 182: Train Loss = 0.007745278479401456, Val Loss = 0.00780246309004724, Sys Error = 0.004302602726966143, Rand Error = 0.08662057667970657\n",
      "Epoch 183: Train Loss = 0.007820379497959863, Val Loss = 0.00808383896946907, Sys Error = 0.018359098583459854, Rand Error = 0.08646657317876816\n",
      "Epoch 184: Train Loss = 0.007611626125655548, Val Loss = 0.00791601960081607, Sys Error = -0.007488013245165348, Rand Error = 0.08703027665615082\n",
      "Epoch 185: Train Loss = 0.007735580871921293, Val Loss = 0.00821949860546738, Sys Error = 0.021920131519436836, Rand Error = 0.08642210066318512\n",
      "Epoch 186: Train Loss = 0.007712544429354196, Val Loss = 0.008029221231117845, Sys Error = 0.01680128276348114, Rand Error = 0.08645346760749817\n",
      "Epoch 187: Train Loss = 0.007755050636004917, Val Loss = 0.007858114433474839, Sys Error = 0.009625186212360859, Rand Error = 0.08653299510478973\n",
      "Epoch 188: Train Loss = 0.007684626448682921, Val Loss = 0.008439852902665735, Sys Error = 0.026505013927817345, Rand Error = 0.08643587678670883\n",
      "Epoch 189: Train Loss = 0.007795018520271189, Val Loss = 0.00789045337587595, Sys Error = 0.01120054628700018, Rand Error = 0.08654280006885529\n",
      "Epoch 190: Train Loss = 0.007651534272617725, Val Loss = 0.00783475290518254, Sys Error = 0.008203157223761082, Rand Error = 0.08653634041547775\n",
      "Epoch 191: Train Loss = 0.007744221141301962, Val Loss = 0.007840416335966438, Sys Error = -0.004167089704424143, Rand Error = 0.0868157222867012\n",
      "Epoch 192: Train Loss = 0.007697878589550423, Val Loss = 0.008036123192869126, Sys Error = 0.01697813905775547, Rand Error = 0.08646583557128906\n",
      "Epoch 193: Train Loss = 0.007562746954432061, Val Loss = 0.007945790886878967, Sys Error = 0.013777563348412514, Rand Error = 0.08650194108486176\n",
      "Epoch 194: Train Loss = 0.007749788316919706, Val Loss = 0.0078057566657662395, Sys Error = 0.0042152805253863335, Rand Error = 0.08665032684803009\n",
      "Epoch 195: Train Loss = 0.007691853630984592, Val Loss = 0.007905993144959211, Sys Error = 0.012186041101813316, Rand Error = 0.08649975061416626\n",
      "Epoch 196: Train Loss = 0.00780310781217765, Val Loss = 0.007829291850794107, Sys Error = -0.0033803724218159914, Rand Error = 0.08678972721099854\n",
      "Epoch 197: Train Loss = 0.007758608586046585, Val Loss = 0.007799093751236797, Sys Error = 0.0011544956360012293, Rand Error = 0.08669129759073257\n",
      "Epoch 198: Train Loss = 0.007726391620370884, Val Loss = 0.007806953741237521, Sys Error = -0.001681204535998404, Rand Error = 0.08670982718467712\n",
      "Epoch 199: Train Loss = 0.007618371375598187, Val Loss = 0.007919071638025344, Sys Error = 0.01239075232297182, Rand Error = 0.0865626186132431\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ✅ Define function for testing the model\n",
    "def test_network(model, testloader, loss_module, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    SD_test = 0\n",
    "    sys_test = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x in testloader:\n",
    "            batch = x[0].to(device)\n",
    "            f_ref = torch.flatten(x[2].to(device))\n",
    "            \n",
    "            f_pred = torch.flatten(model(batch))\n",
    "            \n",
    "            loss_f = loss_module(f_pred, f_ref)\n",
    "            test_loss += loss_f.item()\n",
    "            \n",
    "            SD, sys = hf.error_metrics(f_pred.cpu().numpy(), f_ref.cpu().numpy())\n",
    "            SD_test += SD**2\n",
    "            sys_test += sys\n",
    "\n",
    "    SD_test = np.sqrt(SD_test / len(testloader))\n",
    "    sys_test = sys_test / len(testloader)\n",
    "\n",
    "    return test_loss / len(testloader), SD_test, sys_test\n",
    "\n",
    "# ✅ Define function for creating the model\n",
    "def make_model(n_inputs=5, n_hidden=1, n_outputs=1):\n",
    "    model = nn.Sequential()\n",
    "    for i in range(n_hidden):\n",
    "        model.add_module(f'layer_linear{i}', nn.Linear(n_inputs, n_inputs))\n",
    "        model.add_module(f'layer_ReLu{i}', nn.ReLU())\n",
    "    model.add_module('last_layer', nn.Linear(n_inputs, n_outputs))\n",
    "    model.add_module('last', nn.Sigmoid())  # Ensures positive outputs\n",
    "    model.apply(init_weights)  # Initialize weights\n",
    "    return model\n",
    "\n",
    "# Function for initializing network weights\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)  # Initialize bias\n",
    "\n",
    "# Ensure bvalues is defined\n",
    "bvalues = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ✅ Create model instance (this is required)\n",
    "model = make_model(n_inputs=len(bvalues), n_hidden=2, n_outputs=1)\n",
    "model.to(device)  # Move to GPU if available\n",
    "\n",
    "# Ensure testloader is created\n",
    "trainloader, inferloader, testloader = sim_dat(bvalues, batch_size=16, sims=1000)\n",
    "\n",
    "# Define loss function\n",
    "loss_module = nn.MSELoss(reduction='mean').to(device)\n",
    "\n",
    "# ✅ Train the model\n",
    "train_network('test_run', bvalues=bvalues)  \n",
    "\n",
    "# ✅ Now test the trained model\n",
    "test_loss, SD_test, sys_test = test_network(model, testloader, loss_module, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "D.\tReturn back to SGD. Test how the performance depends on the learning rate. This can be done by plotting the systematic (sys_test) and random (SD_test) errors as function of learning rate (10<LR<0.0000001; steps in order of magnitude; e.g. 10, 1, 0.1, ….).\n",
    "##### Plot the performance (sys_val and SD_val) as function of the learning rate and add this to your report. Explain what you see (hint: take a look at the loss curves).\n",
    "easiest is to save the final performance and plot it in the jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the range of learning rates to test\n",
    "learning_rates = np.logspace(1, -5, num=7)  # 10, 1, 0.1, ..., 0.0000001\n",
    "\n",
    "# Store results\n",
    "sys_errors = []\n",
    "rand_errors = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTraining with learning rate: {lr}\")\n",
    "    \n",
    "    # ✅ Train the model and get validation errors\n",
    "    trained_model, SD_val, sys_val = train_network(f'test_LR_{lr}', learningrate=lr, bvalues=bvalues)\n",
    "\n",
    "    # ✅ Store results\n",
    "    sys_errors.append(sys_val)\n",
    "    rand_errors.append(SD_val)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(learning_rates, sys_errors, marker='o', label='Systematic Error')\n",
    "plt.plot(learning_rates, rand_errors, marker='s', label='Random Error')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Error vs Learning Rate for SGD')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "E.\tPlot the performance (sys_val and SD_val) as function of the width (number of neurons per layer: 5, 10, 20, 50, 100) and depth (number of hidden layers: 1, 2, ..., 8) of the network and add these to the report. Discuss how width and depth may influence the network; if it has this behaviour in your data, highlight it; if not, explain why it may not occur in your dataset.\n",
    "\n",
    "Hint: currently, the width of the network copies the width of the data, so you need to uncouple the input width from the network width by adapting the \"make model\" code.\n",
    "Tip: you only need to plot different widths for 1 depth (e.g. 2) and different depths for 1 width (e.g. 10).\n",
    "Tip: possibly some effects get clearer when more training data is simulated (\"sims\" in data_sim). But also note that having too much training data may hide some of the effects from other exercises, so don't forget to revert it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "F.\tTry different batch sizes (1, 4, 16, 64, 128, 516).Explain the behavior of the network you see.What is the effect of having smaller batches? And larger batches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "G.Chose 3 hidden layers, width of 40,  learning rate of 0.1 and batch size of 2. Now train the network for 3000 epochs. At what point in the network fully trained? How do you see this? Does any overfitting occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Second week (on schedule means finish by Wednesday) [if updated: 12-2-2025]\n",
    "H.\tCurrently, the model uses a Relu activation function. Test the effect of different activation funtions on the network performance. Show how well does a sigmoid or ELU work (i.e. what is the effect on performance)?\n",
    "\n",
    "Note, you can either:\n",
    "- adapt the scripts above to program this \"neatly\" as input parameter.\n",
    "- redefine new \"programs\" below that have the new properties you want.\n",
    "\n",
    "Note that in the case of option 1, your programs need to stay backwards compatible, as examiners will need to be able to rerun your code and reproduce your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So far, we simualted a small amount of data, as it is easier to show overfitting etc. Note that you may want to simulate somewhat more data for this and the following exercises. This can be achieved by setting sims=100000 for the following exercises\n",
    "\n",
    "I.\tLet the network also predict D and Dp (note x[1] and x[3] are D and Dp respectively). Show the loss curve of D and Dp and explain how you can see that they have been implemented properly/the model is learning them.\n",
    "o\tThe network will need more than 1 output --> tip, use loss_D.backward(retain_graph=True) for the first two losses to remember losses and propogate all 3 losses backward in turn\n",
    "o\tAlternatively, you could train 3 networks simultaniously\n",
    "Note that 0<f<1 on avergae is orders of magnitude larger than D and Dp. To ensure all three losses equally affect the network weights you may want to enlarge the loss of D and Dp by multiplying them with some value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "J.\tUse the sigmoid in the final layer to constrain 0.5e-3 <D< 3.5e-3; 0<f<1; 5e-3<D*<130e-3. Explain how you did this.\n",
    "Note: D, f and D* are in very different parameter value ranges, and hence their RMS is too. A network will focus on the largest loss. You may want to scale the RMS to similar ranges for the network to consider all 3 parameters during optmizing.\n",
    "Also note: You may want to play with hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# For scoring 7.5+\n",
    "So far, you have been working with simulated data (taken care of by Data_loader.py). For such data, we know the ground truth values. However, in vivo, we have no way of knowing the ground truth. How will our network perform? Note that for this exercise you may need ot play with hyperparameters and design choices to train.\n",
    "\n",
    "K.\tUse the network, as trained in (J) and apply it to real-world data which is provided by running “dataval, valid_id, bvalues = hf.load_real_data(eval=True)”.\n",
    "- You will need to export you trained network in the return of the \"train_network\" function\n",
    "- you will need to apply it to the \"dataval\" from hf.load_real_data(eval=True)\n",
    "- you will then want to put the outputs through \"hf.plot_example(np.squeeze(D_out), valid_id,0.003)\", \"hf.plot_example(np.squeeze(f_out), valid_id,0.7)\" and \"hf.plot_example(np.squeeze(Dp_out), valid_id,0.1)\", with the predicted D, f and Dp being D_out, f_out and Dp_out.\n",
    "\n",
    "Note that alongside your plot (the first), also a conventional least squares fit is provided as a reference. Show the resulting parameter maps. How does your approach compare? Why do you think your particular approach would look better/worse?\n",
    "\n",
    "L.\tIdeally, you would train your network on real-world data. However, in this particular case, it is hard to get gold standard references. Luckily, we can use our understanding of physics, and of how stuff “should behave” to work our way around this. You will redesign your network loss, such that it can train on data without any gold standard references! Instead of placing the L2 loss on f_pred v.s. f_ref. Currently, the network is learning to minimize the difference between predicted fpred and the ground truth referene ftrue. In vivo, we may not have these references. To overcome this, we will now introduce a physics-informed loss. Use the IVIM equation [1] to propogate the predictions (D, f and Dp) into the signal space (S). Then, take e.g. the mean-square-error between the predicted signal and the input signal. Note that you will need to use torch functions (instead of numpy functions) to ensure you can backpropogate the loss through the equation into the network. You can train this network on the simulated data from earlier exercises. But it should also be able to train it on the in vivo data from \"datatrain, bvalues = hf.load_real_data(eval=False)\". This ensures that the network is use to looking at \"real\" data. Optimize the network’s training using the real data provided (“test_in_vivo.py”; datatrain). Evaluate the network on the same data as in 1 (data, valid_id, bvalues = dl.load_real_data(eval=True)). How does it perform?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datatrain, bvalues = hf.load_real_data(eval=False)\n",
    "dataval, valid_id, bvalues = hf.load_real_data(eval=True)\n",
    "print('training data is ' + str(len(datatrain)) + ' long')\n",
    "\n",
    "''''your code here'''\n",
    "### use datatrain to train your network.\n",
    "## then test your network using dataval.\n",
    "''''if you manage to make predictions of D, f and Dp, the following code will allow you to plot them:'''\n",
    "\n",
    "hf.plot_example(np.squeeze(D_out), valid_id,0.003)\n",
    "hf.plot_example(np.squeeze(f_out), valid_id,0.7)\n",
    "hf.plot_example(np.squeeze(Dp_out), valid_id,0.1)\n",
    "hf.plot_ref()\n",
    "\n",
    "### you can compare supervised and self-supervised fits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
